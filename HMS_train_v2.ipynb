{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ff672",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    new_directory = \"workspace/\"\n",
    "    os.chdir(new_directory)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Import\n",
    "# =============================\n",
    "#system    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import importlib\n",
    "import gc\n",
    "import glob\n",
    "import random\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "#the basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#visualise\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ===============\n",
    "# import mycode\n",
    "# ===============\n",
    "sys.path.append(\"./src\")\n",
    "#utils\n",
    "from utils.utils import pickle_dump,pickle_load,seed_everything,AttrDict\n",
    "from utils.logger import setup_logger, LOGGER\n",
    "# ===============\n",
    "# Read yaml     \n",
    "# ===============\n",
    "from argparse import ArgumentParser\n",
    "try:\n",
    "    parser       = ArgumentParser()\n",
    "    parser.add_argument('--config', type=str)\n",
    "    args         = parser.parse_args()\n",
    "    config_name  = args.config\n",
    "except:\n",
    "    config_name = None\n",
    "    \n",
    "if config_name==None:\n",
    "    with open('./yaml/config_HMS.yaml', 'r') as yml:\n",
    "        config          = yaml.safe_load(yml)\n",
    "else:\n",
    "    with open('./yaml/'+config_name+'.yaml', 'r') as yml:\n",
    "        config          = yaml.safe_load(yml)\n",
    "\n",
    "# ===============\n",
    "# Path_Base     \n",
    "# ===============\n",
    "EXP_ID                  = config['EXP_ID']\n",
    "LOGGER_dir              = config['path']['LOGGER_dir']\n",
    "LOGGER_PATH             = LOGGER_dir+'/log_'+EXP_ID+\".txt\"\n",
    "input_dir               = config['path']['input_dir']\n",
    "data_dir                = config['path']['data_dir']\n",
    "output_dir              = config['path']['output_dir']\n",
    "save_output_dir         = f'{output_dir}/{EXP_ID}'\n",
    "os.makedirs(save_output_dir,exist_ok=True)\n",
    "\n",
    "input_type              = config['dataset']['input_type']\n",
    "if config['model']['model_type'] =='LOAD':\n",
    "    model_type          = config['model']['model_type']\n",
    "else:\n",
    "    if input_type == 'SPEC':\n",
    "        model_type      = 'CNN'\n",
    "    elif input_type == 'EEG_WAVE':\n",
    "        model_type      = 'WAVE_CNN'\n",
    "    elif input_type == 'EEG_IMG':\n",
    "        model_type      = 'CNN'\n",
    "    elif input_type == 'ALL':\n",
    "        model_type      = 'MULTI'\n",
    "    config['model']['model_type']   = model_type\n",
    "\n",
    "# ===============\n",
    "# utils\n",
    "# ===============\n",
    "Debug                               = config['calc_mode']['Debug']\n",
    "if Debug:\n",
    "    config['train']['EPOCHS']          = 1\n",
    "    config['train']['BATCH_SIZE_Tr']   = 2\n",
    "    config['train']['BATCH_SIZE_Val']  = 2\n",
    "    config['calc_mode']['Split_Mode']  = 'def'\n",
    "    config['calc_mode']['NFOLDS']      = 1\n",
    "    config['calc_mode']['Calc_Fold']   = [0]  \n",
    "seed                    = config['train']['SEED']\n",
    "seed_everything(seed)\n",
    "# ===============\n",
    "# LOGGER\n",
    "# ===============\n",
    "setup_logger(out_file=LOGGER_PATH)\n",
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# ===============\n",
    "# SAVE\n",
    "# ===============\n",
    "os.makedirs(f'{save_output_dir}/yaml',exist_ok=True)\n",
    "with open(f'{save_output_dir}/yaml/config_HMS.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "shutil.copytree('./src', f'{save_output_dir}/src', dirs_exist_ok=True)\n",
    "config             = AttrDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess.preprocess\n",
    "importlib.reload(preprocess.preprocess)\n",
    "from preprocess.preprocess import read_rawdata\n",
    "#====params====#\n",
    "DEVICE                          = config['train']['DEVICE']\n",
    "num_Debug                       = config['calc_mode']['num_Debug']\n",
    "\n",
    "\n",
    "calc_stage                          = config['calc_mode']['calc_stage']\n",
    "if calc_stage == 'stage1':\n",
    "    config['calc_mode']['select_train_data']   = 'All'\n",
    "    config['calc_mode']['select_valid_data']   = 'All'\n",
    "    config['calc_mode']['calc_valid_result']   = ['All','flg1_vote']\n",
    "    config['calc_mode']['judge_valid_result']  = 'All'\n",
    "    config['calc_mode']['mode_usepretrain']    = False\n",
    "elif calc_stage == 'stage2':\n",
    "    config['calc_mode']['select_train_data']   = 'flg1_vote' \n",
    "    config['calc_mode']['select_valid_data']   = 'All'\n",
    "    config['calc_mode']['calc_valid_result']   = ['All','flg1_vote']\n",
    "    config['calc_mode']['judge_valid_result']  = 'flg1_vote'\n",
    "    config['calc_mode']['mode_usepretrain']    = True\n",
    "\n",
    "#=更新=#\n",
    "mode_usepretrain                = config['calc_mode']['mode_usepretrain']\n",
    "if mode_usepretrain:\n",
    "    config['dataset']['SPEC']['augmentation']['prob']               = config['dataset']['SPEC']['augmentation']['prob_pretrain']\n",
    "    config['dataset']['SPEC']['augmentation']['stop_aug_epoch']     = config['dataset']['SPEC']['augmentation']['stop_aug_epoch_pretrain']\n",
    "    config['dataset']['SPEC']['mixup']['prob']                      = config['dataset']['SPEC']['mixup']['prob_pretrain'] \n",
    "    config['dataset']['SPEC']['mixup']['alpha']                     = config['dataset']['SPEC']['mixup']['alpha_pretrain']\n",
    "    config['dataset']['SPEC']['mixup']['stop_mixup_epoch']          = config['dataset']['SPEC']['mixup']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['SPEC']['time_cutmix']['prob']                = config['dataset']['SPEC']['time_cutmix']['prob_pretrain']\n",
    "    config['dataset']['SPEC']['time_cutmix']['alpha']               = config['dataset']['SPEC']['time_cutmix']['alpha_pretrain']\n",
    "    config['dataset']['SPEC']['time_cutmix']['stop_mixup_epoch']    = config['dataset']['SPEC']['time_cutmix']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['EEG']['augmentation']['prob']                = config['dataset']['EEG']['augmentation']['prob_pretrain']\n",
    "    config['dataset']['EEG']['augmentation']['stop_aug_epoch']      = config['dataset']['EEG']['augmentation']['stop_aug_epoch_pretrain']\n",
    "    config['dataset']['EEG']['mixup']['prob']                       = config['dataset']['EEG']['mixup']['prob_pretrain']\n",
    "    config['dataset']['EEG']['mixup']['alpha']                      = config['dataset']['EEG']['mixup']['alpha_pretrain']\n",
    "    config['dataset']['EEG']['mixup']['stop_mixup_epoch']           = config['dataset']['EEG']['mixup']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['EEG']['time_cutmix']['prob']                 = config['dataset']['EEG']['time_cutmix']['prob_pretrain']\n",
    "    config['dataset']['EEG']['time_cutmix']['alpha']                = config['dataset']['EEG']['time_cutmix']['alpha_pretrain']\n",
    "    config['dataset']['EEG']['time_cutmix']['stop_mixup_epoch']     = config['dataset']['EEG']['time_cutmix']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['EEG_IMG']['augmentation']['prob']            = config['dataset']['EEG_IMG']['augmentation']['prob_pretrain']\n",
    "    config['dataset']['EEG_IMG']['augmentation']['stop_aug_epoch']  = config['dataset']['EEG_IMG']['augmentation']['stop_aug_epoch_pretrain']\n",
    "    config['dataset']['EEG_IMG']['mixup']['prob']                   = config['dataset']['EEG_IMG']['mixup']['prob_pretrain']\n",
    "    config['dataset']['EEG_IMG']['mixup']['alpha']                  = config['dataset']['EEG_IMG']['mixup']['alpha_pretrain']\n",
    "    config['dataset']['EEG_IMG']['mixup']['stop_mixup_epoch']       = config['dataset']['EEG_IMG']['mixup']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['EEG_IMG']['time_cutmix']['prob']             = config['dataset']['EEG_IMG']['time_cutmix']['prob_pretrain']\n",
    "    config['dataset']['EEG_IMG']['time_cutmix']['alpha']            = config['dataset']['EEG_IMG']['time_cutmix']['alpha_pretrain']\n",
    "    config['dataset']['EEG_IMG']['time_cutmix']['stop_mixup_epoch'] = config['dataset']['EEG_IMG']['time_cutmix']['stop_mixup_epoch_pretrain']\n",
    "    config['train']['EPOCHS_SETTING']['SPEC']                       = config['train']['EPOCHS_SETTING']['SPEC_pretrain']\n",
    "    config['train']['EPOCHS_SETTING']['EEG_WAVE']                   = config['train']['EPOCHS_SETTING']['EEG_WAVE_pretrain']\n",
    "    config['train']['EPOCHS_SETTING']['EEG_IMG']                    = config['train']['EPOCHS_SETTING']['EEG_IMG_pretrain']\n",
    "    config['train']['OPTIMIZER']['LEARNING_RATE_SPEC']              = config['train']['OPTIMIZER']['LEARNING_RATE_SPEC_pretrain']\n",
    "    config['train']['OPTIMIZER']['LEARNING_RATE_EEG_WAVE']          = config['train']['OPTIMIZER']['LEARNING_RATE_EEG_WAVE_pretrain']\n",
    "    config['train']['OPTIMIZER']['LEARNING_RATE_EEG_IMG']           = config['train']['OPTIMIZER']['LEARNING_RATE_EEG_IMG_pretrain']\n",
    "    #ALL\n",
    "    config['dataset']['ALL']['augmentation']['prob']               = config['dataset']['ALL']['augmentation']['prob_pretrain']\n",
    "    config['dataset']['ALL']['augmentation']['stop_aug_epoch']     = config['dataset']['ALL']['augmentation']['stop_aug_epoch_pretrain']\n",
    "    config['dataset']['ALL']['mixup']['prob']                      = config['dataset']['ALL']['mixup']['prob_pretrain'] \n",
    "    config['dataset']['ALL']['mixup']['alpha']                     = config['dataset']['ALL']['mixup']['alpha_pretrain']\n",
    "    config['dataset']['ALL']['mixup']['stop_mixup_epoch']          = config['dataset']['ALL']['mixup']['stop_mixup_epoch_pretrain']\n",
    "    config['dataset']['ALL']['time_cutmix']['prob']                = config['dataset']['ALL']['time_cutmix']['prob_pretrain']\n",
    "    config['dataset']['ALL']['time_cutmix']['alpha']               = config['dataset']['ALL']['time_cutmix']['alpha_pretrain']\n",
    "    config['dataset']['ALL']['time_cutmix']['stop_mixup_epoch']    = config['dataset']['ALL']['time_cutmix']['stop_mixup_epoch_pretrain']\n",
    "    config['train']['EPOCHS_SETTING']['ALL']                       = config['train']['EPOCHS_SETTING']['ALL_pretrain']\n",
    "    config['train']['OPTIMIZER']['LEARNING_RATE_ALL']              = config['train']['OPTIMIZER']['LEARNING_RATE_ALL_pretrain']\n",
    "\n",
    "\n",
    "#====Read Data====#\n",
    "train_meta                      = pd.read_csv(data_dir + \"/train.csv\")\n",
    "files_train_eegs                = glob.glob(f'{data_dir}/train_eegs/*')\n",
    "files_train_eegs_names          = [int(os.path.basename(file).split('.')[0]) for file in files_train_eegs]\n",
    "files_train_spectrograms        = glob.glob(f'{data_dir}/train_spectrograms/*')\n",
    "files_train_spectrograms_names  = [int(os.path.basename(file).split('.')[0]) for file in files_train_spectrograms]\n",
    "\n",
    "#=====split_data=====#\n",
    "if (config['calc_mode']['Split_Mode'] == 'TH_folds1'):\n",
    "    train_meta_TH               = pd.read_csv(data_dir + \"/folds1.csv\")\n",
    "elif (config['calc_mode']['Split_Mode'] == 'TH_folds2'):\n",
    "    train_meta_TH               = pd.read_csv(data_dir + \"/folds2.csv\")\n",
    "train_meta_TH                   = train_meta_TH.rename(columns={'fold':'fold_TH'})\n",
    "train_meta                      = pd.merge(train_meta,train_meta_TH[['label_id','fold_TH','eeg_nan_ratio','spec_nan_ratio','eeg_std_0']],on='label_id',how='left')\n",
    "train_meta['fold']              = train_meta['fold_TH'].values\n",
    "\n",
    "\n",
    "#====Read RawData====#\n",
    "train_eegs_data_dict,len_train_eegs_data_dict,col_eegs,\\\n",
    "dict_col_index,list_max_eeg,list_min_eeg,list_nan_eeg_id,\\\n",
    "train_spectrograms_data_dict,len_train_spectrograms_data_dict,\\\n",
    "col_spectrograms,dict_col_index,list_nan_spec_id               = read_rawdata(config,data_dir,output_dir,input_type,\\\n",
    "                                                                              files_train_eegs_names,files_train_spectrograms_names)\n",
    "\n",
    "#=====Add Select Flg=====#\n",
    "th_vote                             = config['calc_mode']['thresh_total_vote']\n",
    "col_labels                          = config['dataset']['col_labels']\n",
    "col_labels_ratio                    = [f'{col}_ratio' for col in col_labels]\n",
    "train_meta['sum_vote']              = train_meta[col_labels].sum(axis=1)\n",
    "for col in col_labels:\n",
    "    train_meta[f'{col}_ratio']      = train_meta[col]/train_meta['sum_vote']\n",
    "    \n",
    "#==Flg1:sum_vote&maxratio dict_labelid2sumvote,dict_labelid2maxratio==#\n",
    "select_rule1                        = (train_meta['sum_vote']>=th_vote)\n",
    "train_meta['flg1_vote']             = select_rule1\n",
    "dict_labelid2flg1vote               = train_meta.set_index('label_id')['flg1_vote'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac968b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer.datasets\n",
    "importlib.reload(trainer.datasets)\n",
    "from trainer.datasets import HMS_Dataset\n",
    "#====dataset_save_mode EEG_IMG====#\n",
    "dataset_mode_EEG_IMG                    = config['dataset']['EEG_IMG']['dataset_mode'] #'load','pass'\n",
    "save_eeg_img                            = (input_type=='EEG_IMG' and (dataset_mode_EEG_IMG=='save')) or\\\n",
    "                                          (input_type=='ALL'     and (dataset_mode_EEG_IMG=='save'))\n",
    "if save_eeg_img:\n",
    "    LOGGER.info(f'============Save EEG_IMG============')\n",
    "    tmp_select_data                     = config['dataset']['select_data']\n",
    "    config['dataset']['select_data']    = 'use_alldata'\n",
    "    save_dataset_dir                    = f'{output_dir}/{EXP_ID}/dataset'\n",
    "    os.makedirs(save_dataset_dir,exist_ok=True) \n",
    "    save_dataset                        = HMS_Dataset(train_meta,train_eegs_data_dict,train_spectrograms_data_dict,dict_col_index,\n",
    "                                          config,phase='valid')\n",
    "    save_dataset.update_for_epoch(0)\n",
    "    dict_EEG_IMG                        = dict()\n",
    "    for idx in range(len(save_dataset)):\n",
    "        if idx%1000==0:\n",
    "            LOGGER.info(f'idx:{idx}')\n",
    "        eeg_id                          = save_dataset[idx]['eeg_id']\n",
    "        dict_EEG_IMG[eeg_id]            = save_dataset[idx]['sub_eeg_data_img']\n",
    "    pickle_dump(dict_EEG_IMG,f'{save_dataset_dir}/dict_EEG_IMG.pkl')\n",
    "    #更新\n",
    "    config['dataset']['EEG_IMG']['dataset_mode']   = 'load'\n",
    "    config['dataset']['select_data']               = tmp_select_data\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess.preprocess\n",
    "import trainer.datasets\n",
    "import trainer.train\n",
    "import models.model_HMS_CNN\n",
    "import models.model_HMS_WAVE_CNN\n",
    "import models.model_HMS_MULTI\n",
    "importlib.reload(preprocess.preprocess)\n",
    "importlib.reload(trainer.datasets)\n",
    "importlib.reload(trainer.train)\n",
    "importlib.reload(models.model_HMS_CNN)\n",
    "importlib.reload(models.model_HMS_WAVE_CNN)\n",
    "importlib.reload(models.model_HMS_MULTI)\n",
    "from trainer.datasets import HMS_Dataset\n",
    "from trainer.train import calc_train,get_fig\n",
    "from models.model_HMS_CNN import HMSModel_CNN\n",
    "from models.model_HMS_WAVE_CNN import HMSModel_WAVE_CNN\n",
    "from models.model_HMS_MULTI import HMSModel_MULTI\n",
    "from trainer.metrics  import score\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def select_df(df_fold,select_setting):\n",
    "    if select_setting == 'All':\n",
    "        df_fold                 = df_fold.copy()\n",
    "    elif select_setting=='flg1_vote':\n",
    "        df_fold                 = df_fold[df_fold['flg1_vote']==True].reset_index(drop=True)\n",
    "    #flg1_voteがTrueのものがあるeeg_idはTrueだけ残す\n",
    "    df_fold['has_true']         = df_fold.groupby('eeg_id')['flg1_vote'].transform('max')\n",
    "    df_fold                     = df_fold[(df_fold['flg1_vote'] == True) | (df_fold['has_true'] == False)]\n",
    "    df_fold                     = df_fold.drop(columns=['has_true'])\n",
    "    df_fold                     = df_fold.reset_index(drop=True)\n",
    "    return df_fold\n",
    "\n",
    "#=setting=#\n",
    "if input_type == 'SPEC':\n",
    "    config.train.EPOCHS                  = config.train.EPOCHS_SETTING.SPEC\n",
    "    config.train.BATCH_SIZE_Tr           = config.train.BATCH_SIZE_SETTING.SPEC\n",
    "    config.train.BATCH_SIZE_Val          = config.train.BATCH_SIZE_SETTING.SPEC\n",
    "    config.train.OPTIMIZER.LEARNING_RATE = config.train.OPTIMIZER.LEARNING_RATE_SPEC\n",
    "    config.train.OPTIMIZER.WEIGHT_DECAY  = config.train.OPTIMIZER.WEIGHT_DECAY_SPEC\n",
    "elif input_type == 'EEG_IMG':\n",
    "    config.train.EPOCHS                  = config.train.EPOCHS_SETTING.EEG_IMG\n",
    "    config.train.BATCH_SIZE_Tr           = config.train.BATCH_SIZE_SETTING.EEG_IMG\n",
    "    config.train.BATCH_SIZE_Val          = config.train.BATCH_SIZE_SETTING.EEG_IMG\n",
    "    config.train.OPTIMIZER.LEARNING_RATE = config.train.OPTIMIZER.LEARNING_RATE_EEG_IMG\n",
    "    config.train.OPTIMIZER.WEIGHT_DECAY  = config.train.OPTIMIZER.WEIGHT_DECAY_EEG_IMG\n",
    "elif input_type == 'EEG_WAVE':\n",
    "    config.train.EPOCHS                  = config.train.EPOCHS_SETTING.EEG_WAVE\n",
    "    config.train.BATCH_SIZE_Tr           = config.train.BATCH_SIZE_SETTING.EEG_WAVE\n",
    "    config.train.BATCH_SIZE_Val          = config.train.BATCH_SIZE_SETTING.EEG_WAVE\n",
    "    config.train.OPTIMIZER.LEARNING_RATE = config.train.OPTIMIZER.LEARNING_RATE_EEG_WAVE\n",
    "    config.train.OPTIMIZER.WEIGHT_DECAY  = config.train.OPTIMIZER.WEIGHT_DECAY_EEG_WAVE\n",
    "elif input_type == 'ALL':\n",
    "    config.train.EPOCHS                  = config.train.EPOCHS_SETTING.ALL\n",
    "    config.train.BATCH_SIZE_Tr           = config.train.BATCH_SIZE_SETTING.ALL\n",
    "    config.train.BATCH_SIZE_Val          = config.train.BATCH_SIZE_SETTING.ALL\n",
    "    config.train.OPTIMIZER.LEARNING_RATE = config.train.OPTIMIZER.LEARNING_RATE_ALL\n",
    "    config.train.OPTIMIZER.WEIGHT_DECAY  = config.train.OPTIMIZER.WEIGHT_DECAY_ALL\n",
    "\n",
    "#====train====#\n",
    "eeg_nan_ratio_thresh_train  = config['calc_mode']['eeg_nan_ratio_thresh_train']\n",
    "\n",
    "#=Main=#\n",
    "history                     = defaultdict(list) \n",
    "idx_train_folds             = []\n",
    "idx_valid_folds             = []\n",
    "select_train_data           = config['calc_mode']['select_train_data']\n",
    "select_valid_data           = config['calc_mode']['select_valid_data']\n",
    "judge_valid_result          = config['calc_mode']['judge_valid_result']\n",
    "for id_fold in config.calc_mode.Calc_Fold:\n",
    "    LOGGER.info(f'============Fold:{id_fold} Start============')\n",
    "    save_output_dir_fold    = f'{save_output_dir}/fold_{id_fold}'\n",
    "    os.makedirs(save_output_dir_fold,exist_ok=True)\n",
    "    #===split_data===#\n",
    "    idx_train               = train_meta[train_meta['fold']!=id_fold].index.values\n",
    "    idx_valid               = train_meta[train_meta['fold']==id_fold].index.values\n",
    "    df_fold_train           = train_meta.iloc[idx_train].reset_index(drop=True)\n",
    "    df_fold_valid           = train_meta.iloc[idx_valid].reset_index(drop=True)\n",
    "\n",
    "    #===select_data_train===#\n",
    "    df_fold_train           = df_fold_train[df_fold_train[\"eeg_nan_ratio\"] <= eeg_nan_ratio_thresh_train]\n",
    "    df_fold_train           = df_fold_train[df_fold_train[\"eeg_std_0\"] == False].reset_index(drop=True)\n",
    "    df_fold_valid           = df_fold_valid[df_fold_valid[\"eeg_std_0\"] == False].reset_index(drop=True)\n",
    "\n",
    "    #===select_data===#\n",
    "\n",
    "    df_fold_train           = select_df(df_fold_train,select_train_data)\n",
    "    df_fold_valid           = select_df(df_fold_valid,select_valid_data)\n",
    "    df_fold_valid_judge     = select_df(df_fold_valid,judge_valid_result)\n",
    "    \n",
    "    #===dataset===#\n",
    "    train_dataset           = HMS_Dataset(df_fold_train,train_eegs_data_dict,train_spectrograms_data_dict,dict_col_index,\n",
    "                                          config,phase='train')\n",
    "    valid_dataset           = HMS_Dataset(df_fold_valid,train_eegs_data_dict,train_spectrograms_data_dict,dict_col_index,\n",
    "                                          config,phase='valid')\n",
    "    train_dataset.update_for_epoch(0)\n",
    "    valid_dataset.update_for_epoch(0)\n",
    "    train_loader            = DataLoader(train_dataset, batch_size=config.train.BATCH_SIZE_Tr, shuffle=True,\n",
    "                                         num_workers=config.train.NUM_WORKERS,pin_memory=True)\n",
    "    valid_loader            = DataLoader(valid_dataset, batch_size=config.train.BATCH_SIZE_Val, shuffle=False,\n",
    "                                         num_workers=config.train.NUM_WORKERS,pin_memory=True)\n",
    "    \n",
    "    #===model===#\n",
    "    if model_type == 'CNN':\n",
    "        model               = HMSModel_CNN(config,input_type=input_type)\n",
    "    elif model_type == 'WAVE_CNN':\n",
    "        model               = HMSModel_WAVE_CNN(config)\n",
    "    elif model_type == 'MULTI':\n",
    "        model               = HMSModel_MULTI(config,id_fold=id_fold)\n",
    "    elif model_type == 'LOAD':\n",
    "        exp_id_load         = config['model']['LOAD']['exp_id_load']\n",
    "        path_model_load     = f'{output_dir}/{exp_id_load}/fold_{id_fold}/best.pt'\n",
    "        model               = torch.load(path_model_load)\n",
    "        load_model_type     = model.config.model.model_type\n",
    "        config['model']['model_type'] = load_model_type\n",
    "        LOGGER.info(f\"======Model Load======\")\n",
    "        LOGGER.info(f\"===exp_id_load={exp_id_load}===\")\n",
    "    model                   = model.to(DEVICE)\n",
    "    \n",
    "    #===loss===#\n",
    "    loss_function           = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    loss_function           = loss_function.to(DEVICE)\n",
    "    \n",
    "    #===optimizerl===#\n",
    "    if (input_type == 'SPEC')&(config['model']['CNN_SPEC']['ch_head']=='cnn2d'):\n",
    "        special_params      = list(model.backbone_ch_head.parameters())\n",
    "        special_params_ids  = {id(p) for p in special_params}\n",
    "        base_params         = [p for p in model.parameters() if id(p) not in special_params_ids]\n",
    "        optimizer           = torch.optim.Adam(\n",
    "                                [\n",
    "                                    {\"params\": base_params, \"lr\": config.train.OPTIMIZER.LEARNING_RATE},\n",
    "                                    {\"params\": special_params, \"lr\": 0.03*config.train.OPTIMIZER.LEARNING_RATE},#0.03\n",
    "                                ],\n",
    "                                weight_decay=config.train.OPTIMIZER.WEIGHT_DECAY,)\n",
    "    elif (input_type == 'EEG_IMG')&(config['model']['CNN_EEG']['ch_head']=='cnn2d'):\n",
    "        special_params      = list(model.backbone_ch_head.parameters())\n",
    "        special_params_ids  = {id(p) for p in special_params}\n",
    "        base_params         = [p for p in model.parameters() if id(p) not in special_params_ids]\n",
    "        optimizer           = torch.optim.Adam(\n",
    "                                [\n",
    "                                    {\"params\": base_params, \"lr\": config.train.OPTIMIZER.LEARNING_RATE},\n",
    "                                    {\"params\": special_params, \"lr\": 0.03*config.train.OPTIMIZER.LEARNING_RATE},#0.03\n",
    "                                ],\n",
    "                                weight_decay=config.train.OPTIMIZER.WEIGHT_DECAY,)\n",
    "    elif (input_type == 'EEG_WAVE')&(config['model']['WAVE_EEG_CNN']['ch_head']=='cnn2d'):\n",
    "        special_params      = list(model.backbone_ch_head.parameters())\n",
    "        special_params_ids  = {id(p) for p in special_params}\n",
    "        base_params         = [p for p in model.parameters() if id(p) not in special_params_ids]\n",
    "        optimizer           = torch.optim.Adam(\n",
    "                                [\n",
    "                                    {\"params\": base_params, \"lr\": config.train.OPTIMIZER.LEARNING_RATE,\"weight_decay\":1.e-7},\n",
    "                                    {\"params\": special_params, \"lr\": 0.06*config.train.OPTIMIZER.LEARNING_RATE,\"weight_decay\":1.e-7},#0.03\n",
    "                                ],\n",
    "                                weight_decay=config.train.OPTIMIZER.WEIGHT_DECAY,)\n",
    "        \n",
    "    elif (input_type == 'ALL'):\n",
    "        special_params1 = list(model.model_spec.backbone_ch_head.parameters())\n",
    "        special_params2 = list(model.model_eeg_wave.backbone_ch_head.parameters())\n",
    "        special_params3 = list(model.model_eeg_img.backbone_ch_head.parameters())\n",
    "\n",
    "        special_params_ids1 = {id(p) for p in special_params1}\n",
    "        special_params_ids2 = {id(p) for p in special_params2}\n",
    "        special_params_ids3 = {id(p) for p in special_params3}\n",
    "\n",
    "        base_params = [p for p in model.parameters() if id(p) not in special_params_ids1 and id(p) not in special_params_ids2 and id(p) not in special_params_ids3]\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": base_params,     \"lr\": config.train.OPTIMIZER.LEARNING_RATE, \"weight_decay\": 1.e-7},\n",
    "                {\"params\": special_params1, \"lr\": 0.03 * config.train.OPTIMIZER.LEARNING_RATE, \"weight_decay\": 1.e-7},\n",
    "                {\"params\": special_params2, \"lr\": 0.06 * config.train.OPTIMIZER.LEARNING_RATE, \"weight_decay\": 1.e-7},\n",
    "                {\"params\": special_params3, \"lr\": 0.03 * config.train.OPTIMIZER.LEARNING_RATE, \"weight_decay\": 1.e-7},\n",
    "            ],\n",
    "            weight_decay=config.train.OPTIMIZER.WEIGHT_DECAY,\n",
    "        )\n",
    "    else:\n",
    "        optimizer               = torch.optim.Adam(model.parameters(),\n",
    "                                                lr=config.train.OPTIMIZER.LEARNING_RATE,\n",
    "                                                weight_decay=config.train.OPTIMIZER.WEIGHT_DECAY)\n",
    "    \n",
    "    #===schedulerl===#\n",
    "    max_steps               = config.train.EPOCHS * len(train_loader)\n",
    "    warmup_steps            = 0\n",
    "    scheduler               = get_cosine_schedule_with_warmup(optimizer, num_training_steps=max_steps,\n",
    "                                                                num_warmup_steps=warmup_steps)\n",
    "\n",
    "    LOGGER.info('======Start_Train======')\n",
    "    history_fold            = dict()\n",
    "    history_fold            = calc_train(model, config, train_dataset, valid_dataset,train_loader, valid_loader,\n",
    "                                         optimizer,scheduler,loss_function,save_output_dir_fold,LOGGER,history_fold,\n",
    "                                         df_fold_valid)\n",
    "    # ===============\n",
    "    # save_fig\n",
    "    # =============== \n",
    "    get_fig(history_fold,save_output_dir_fold,config)\n",
    "    # ===============\n",
    "    # result_fold\n",
    "    # =============== \n",
    "    LOGGER.info(f'============Fold:{id_fold} END============')\n",
    "    history[id_fold]        = history_fold\n",
    "    history_fold['best_df_valid_output'].to_csv(f'{save_output_dir_fold}/df_valid_output.csv')\n",
    "    history_fold['best_df_valid_labels'].to_csv(f'{save_output_dir_fold}/df_valid_labels.csv')\n",
    "    LOGGER.info('Best_Ep ' + str(history_fold['best_epoch']) +  ',Best_val_loss: ' + str(np.round(history_fold['best_valid_losses'],4))+\n",
    "                ',Best_val_metrics: ' + str(np.round(history_fold['best_valid_metrics'],4)))\n",
    "    formatted_mae = [f'{val:.2f}' for val in history_fold['best_valid_MAE']]\n",
    "    formatted_acc = [f'{val:.1f}' for val in history_fold['best_valid_ACC']]\n",
    "    LOGGER.info(f'======Best_MAE : {formatted_mae}======')\n",
    "    LOGGER.info(f'======Best_ACC : {formatted_acc}======')\n",
    "    #folds\n",
    "    history_fold['idx_train'] = idx_train\n",
    "    history_fold['idx_valid'] = idx_valid\n",
    "    idx_train_folds.append(idx_train)\n",
    "    idx_valid_folds.append(idx_valid)\n",
    "\n",
    "    # ===============\n",
    "    # OOF_fold\n",
    "    # =============== \n",
    "    LOGGER.info(f'============Fold:{id_fold} END============')\n",
    "    del train_dataset,valid_dataset,train_loader,valid_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "# ===============\n",
    "# result_all\n",
    "# =============== \n",
    "#===result_all===#\n",
    "best_epochs             = []\n",
    "folds                   = []\n",
    "best_train_losses       = []\n",
    "best_valid_losses       = []\n",
    "best_valid_metrics      = []\n",
    "best_valid_maes         = []\n",
    "best_valid_accs         = []\n",
    "for idx,id_fold in enumerate(config.calc_mode.Calc_Fold):\n",
    "    folds.append(int(id_fold))\n",
    "    best_epochs.append(int(history[id_fold]['best_epoch']))\n",
    "    best_train_losses.append(history[id_fold]['best_train_losses'])\n",
    "    best_valid_losses.append(history[id_fold]['best_valid_losses'])\n",
    "    best_valid_metrics.append(history[id_fold]['best_valid_metrics'])\n",
    "    best_valid_maes.append(history[id_fold]['best_valid_MAE'])\n",
    "    best_valid_accs.append(history[id_fold]['best_valid_ACC'])\n",
    "df_result                               = pd.DataFrame()\n",
    "df_result['folds']                      = folds\n",
    "df_result['best_epochs']                = best_epochs\n",
    "df_result['best_train_losse']           = best_train_losses\n",
    "df_result['best_valid_losse']           = best_valid_losses\n",
    "df_result['best_valid_metrics']         = best_valid_metrics\n",
    "df_result                               = df_result.T\n",
    "df_result.to_csv(f'{save_output_dir}/df_result.csv')\n",
    "\n",
    "#=df_oof=#\n",
    "for idx,id_fold in enumerate(config.calc_mode.Calc_Fold):\n",
    "    if idx==0:\n",
    "        df_oof_output       = history[id_fold]['best_df_valid_output']\n",
    "        df_oof_labels       = history[id_fold]['best_df_valid_labels']\n",
    "    else:\n",
    "        df_oof_output       = pd.concat([df_oof_output,history[id_fold]['best_df_valid_output']],axis=0).reset_index(drop=True)\n",
    "        df_oof_labels       = pd.concat([df_oof_labels,history[id_fold]['best_df_valid_labels']],axis=0).reset_index(drop=True)\n",
    "\n",
    "df_oof_output_score         = df_oof_output[config['dataset']['col_labels']]\n",
    "df_oof_labels_score         = df_oof_labels[config['dataset']['col_labels']]\n",
    "df_oof_output_score['id']   = np.arange(len(df_oof_output_score))\n",
    "df_oof_labels_score['id']   = np.arange(len(df_oof_labels_score))\n",
    "oof_metrics                 = score(solution=df_oof_labels_score.copy(), submission=df_oof_output_score.copy(), \n",
    "                                                row_id_column_name='id')\n",
    "\n",
    "df_oof_output.to_csv(f'{save_output_dir}/df_oof_output.csv')\n",
    "df_oof_labels.to_csv(f'{save_output_dir}/df_oof_labels.csv')\n",
    "LOGGER.info('Best_Valid_Metrics_OOF    : '+str(np.round(oof_metrics,4)))\n",
    "LOGGER.info('Best_Valid_Metrics_FoldAve: '+str(np.round(np.mean(best_valid_metrics),4)))\n",
    "\n",
    "#===other===#\n",
    "proba_pred                              = torch.tensor(df_oof_output[config['dataset']['col_labels']].values)\n",
    "label                                   = torch.tensor(df_oof_labels[config['dataset']['col_labels']].values)\n",
    "mae                                     = abs(proba_pred-label).mean(axis=0).tolist()\n",
    "acc                                     = []\n",
    "predicted_classes                       = torch.argmax(proba_pred, dim=1)\n",
    "label_classes                           = torch.argmax(label, dim=1)\n",
    "for c in range(proba_pred.shape[1]):\n",
    "    class_mask                          = label_classes == c\n",
    "    class_predictions                   = predicted_classes[class_mask]\n",
    "    class_labels                        = label_classes[class_mask]\n",
    "    correct_predictions                 = torch.eq(class_predictions, class_labels)\n",
    "    accuracy                            = torch.mean(correct_predictions.float())\n",
    "    acc.append(accuracy.item() * 100)\n",
    "history['oof_MAE']          = mae\n",
    "history['oof_ACC']          = acc\n",
    "formatted_mae = [f'{val:.2f}' for val in mae]\n",
    "formatted_acc = [f'{val:.1f}' for val in acc]\n",
    "LOGGER.info(f'======oof_MAE : {formatted_mae}======')\n",
    "LOGGER.info(f'======oof_ACC : {formatted_acc}======')\n",
    "\n",
    "#=file_splits=#\n",
    "train_meta.to_csv(f'{save_output_dir}/train_meta.csv')\n",
    "file_splits                 = dict()\n",
    "file_splits['train']        = idx_train_folds\n",
    "file_splits['valid']        = idx_valid_folds\n",
    "pickle_dump(file_splits,f'{save_output_dir}/file_splits.pkl')\n",
    "\n",
    "#=全部=#\n",
    "pickle_dump(history,f'{save_output_dir}/history.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2cddce24a8fc711558e66d9211b62795b81cc4d411009a21a45a14e447841c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
